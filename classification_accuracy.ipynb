{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d04d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "def load_conceptarc_tasks(corpus_dir=\"corpus\"):\n",
    "    tasks = []\n",
    "    for concept in os.listdir(corpus_dir):\n",
    "        concept_path = os.path.join(corpus_dir, concept)\n",
    "        if os.path.isdir(concept_path):\n",
    "            for filename in os.listdir(concept_path):\n",
    "                if filename.endswith(\".json\"):\n",
    "                    filepath = os.path.join(concept_path, filename)\n",
    "                    with open(filepath, \"r\") as f:\n",
    "                        task_data = json.load(f)\n",
    "                    tasks.append({\n",
    "                        \"concept\": concept,\n",
    "                        \"filename\": filename,\n",
    "                        \"task\": task_data\n",
    "                    })\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a7d4f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a visual reasoning and Python programming expert solving ARC-AGI (Abstraction and Reasoning Corpus - Artificial General Intelligence) tasks.\n",
    "\n",
    "Each integer in the grid represents a color:\n",
    "0 = black, 1 = blue, 2 = red, 3 = green, 4 = yellow,\n",
    "5 = grey, 6 = pink, 7 = orange, 8 = light blue, 9 = brown.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c02840",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_prompt = \"\"\"\n",
    "You will receive a set of demonstration pairs (input and output grids) from a visual reasoning task.\n",
    "\n",
    "Your task is to classify the transformation into **one of the following 16 concepts**:\n",
    "\n",
    "- AboveBelow - Objects or patterns are arranged vertically, with relationships defined by what's above or below something else.\n",
    "- Center - Elements are moved to or arranged around the center of the grid.\n",
    "- CleanUp - The task removes noise or extraneous elements to leave a cleaner or more regular structure.\n",
    "- CompleteShape - A partial or broken shape is completed to form a full geometric object.\n",
    "- Copy - A shape or pattern is duplicated, often to another location in the grid.\n",
    "- Count - The number of certain elements is counted to determine placement, output quantity, or transformation.\n",
    "- ExtendToBoundary - Shapes or lines are extended until they touch the edge of the grid.\n",
    "- ExtractObjects - Specific objects are isolated and copied or transformed while others are ignored.\n",
    "- FilledNotFilled - The task distinguishes between filled and hollow shapes or fills in uncolored areas.\n",
    "- HorizontalVertical - Patterns follow or are transformed along horizontal or vertical axes, often involving symmetry or alignment.\n",
    "- InsideOutside - A relationship is determined based on whether elements are inside or outside a defined boundary.\n",
    "- MoveToBoundary - Objects are shifted to the nearest edge of the grid without rotation or change in shape.\n",
    "- Order - Items are rearranged according to size, color, frequency, or another ordinal property.\n",
    "- SameDifferent - Objects are retained or manipulated based on whether they match or differ in some attribute (e.g., color, shape).\n",
    "- TopBottom2D - A flat 2D interpretation of objects where the top and bottom halves of the grid are compared or modified.\n",
    "- TopBottom3D - The task simulates a 3D stacking or layering behavior, such as viewing objects from above or combining vertical slices.\n",
    "\n",
    "Instructions:\n",
    "- Respond ONLY with the exact name of the matching concept from the list above.\n",
    "- Do not explain your answer, just return the concept.\n",
    "- If uncertain, choose the concept that fits best based on the input-output transformations.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68738e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_api_key(file_path=\"key.env\"):\n",
    "    load_dotenv(file_path)\n",
    "    import openai\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not openai.api_key:\n",
    "        print(\"No API key found. Please set OPENAI_API_KEY in key.env.\")\n",
    "    global client\n",
    "    client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import openai\n",
    "\n",
    "def call_gpt(prompt, model=\"o4-mini\", retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                # Only for GPT-4o\n",
    "                # temperature=0.0\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        \n",
    "        except openai.RateLimitError as e:\n",
    "            wait_time = 5 + attempt * 5\n",
    "            print(f\"Rate limit hit. Waiting {wait_time} seconds before retrying...\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "    raise Exception(\"Rate limit retries exhausted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0541ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_prompt_with_task(task_data, base_prompt):\n",
    "    formatted_examples = \"\\n\\nHere are the input-output pairs for the task:\\n\\n\"\n",
    "\n",
    "    for i, pair in enumerate(task_data.get(\"train\", [])):\n",
    "        formatted_examples += f\"Train Input {i+1}: {pair['input']}\\n\"\n",
    "        formatted_examples += f\"Train Output {i+1}: {pair['output']}\\n\\n\"\n",
    "\n",
    "    for i, pair in enumerate(task_data.get(\"test\", [])):\n",
    "        formatted_examples += f\"Test Input {i+1}: {pair['input']}\\n\"\n",
    "        if \"output\" in pair:\n",
    "            formatted_examples += f\"Test Output {i+1}: {pair['output']}\\n\\n\"\n",
    "\n",
    "    full_prompt = base_prompt.strip() + \"\\n\\n\" + formatted_examples.strip()\n",
    "    return full_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d7cad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_concept_classification(corpus_path=\"corpus\"):\n",
    "    conceptarc_tasks = load_conceptarc_tasks(corpus_path)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "\n",
    "    for task in conceptarc_tasks:\n",
    "        full_prompt = create_classification_prompt_with_task(task[\"task\"], classification_prompt)\n",
    "        predicted_concept = call_gpt(full_prompt).strip()\n",
    "\n",
    "        predictions.append({\n",
    "            \"filename\": task[\"filename\"],\n",
    "            \"true_concept\": task[\"concept\"],\n",
    "            \"predicted_concept\": predicted_concept\n",
    "        })\n",
    "\n",
    "        if predicted_concept.lower() == task[\"concept\"].lower():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"\\nAccuracy: {accuracy:.2%} ({correct} out of {total})\")\n",
    "\n",
    "    return predictions, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bec558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 61.25% (98 out of 160)\n",
      "\n",
      "Accuracy per concept:\n",
      "AboveBelow: 5/10 correct (50.00%)\n",
      "Center: 5/10 correct (50.00%)\n",
      "CleanUp: 7/10 correct (70.00%)\n",
      "CompleteShape: 8/10 correct (80.00%)\n",
      "Copy: 9/10 correct (90.00%)\n",
      "Count: 10/10 correct (100.00%)\n",
      "ExtendToBoundary: 10/10 correct (100.00%)\n",
      "ExtractObjects: 9/10 correct (90.00%)\n",
      "FilledNotFilled: 6/10 correct (60.00%)\n",
      "HorizontalVertical: 5/10 correct (50.00%)\n",
      "InsideOutside: 6/10 correct (60.00%)\n",
      "MoveToBoundary: 9/10 correct (90.00%)\n",
      "Order: 6/10 correct (60.00%)\n",
      "SameDifferent: 3/10 correct (30.00%)\n",
      "TopBottom2D: 0/10 correct (0.00%)\n",
      "TopBottom3D: 0/10 correct (0.00%)\n",
      "\n",
      "Total accuracy: 98/160 correct (61.25%)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "load_api_key()\n",
    "predictions, accuracy = evaluate_concept_classification(\"corpus\")\n",
    "\n",
    "total_per_concept = defaultdict(int)\n",
    "correct_per_concept = defaultdict(int)\n",
    "total_all = 0\n",
    "correct_all = 0\n",
    "\n",
    "# Loop through predictions\n",
    "for entry in predictions:\n",
    "    true = entry[\"true_concept\"]\n",
    "    pred = entry[\"predicted_concept\"]\n",
    "    total_per_concept[true] += 1\n",
    "    total_all += 1\n",
    "    if true.lower() == pred.lower():\n",
    "        correct_per_concept[true] += 1\n",
    "        correct_all += 1\n",
    "\n",
    "# Print results\n",
    "print(\"\\nAccuracy per concept:\")\n",
    "for concept in sorted(total_per_concept):\n",
    "    correct = correct_per_concept[concept]\n",
    "    total = total_per_concept[concept]\n",
    "    print(f\"{concept}: {correct}/{total} correct ({(correct / total):.2%})\")\n",
    "\n",
    "overall_accuracy = correct_all / total_all if total_all > 0 else 0\n",
    "print(f\"\\nTotal accuracy: {correct_all}/{total_all} correct ({overall_accuracy:.2%})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
