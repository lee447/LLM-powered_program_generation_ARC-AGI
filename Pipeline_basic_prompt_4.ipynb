{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline: LLM-powered program generation for solving ARC-AGI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import ast\n",
    "import re\n",
    "import importlib.util\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system prompt contains basic information essential for every prompt sent to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a visual reasoning and Python programming expert solving ARC-AGI (Abstraction and Reasoning Corpus - Artificial General Intelligence) tasks.\n",
    "\n",
    "Each integer in the grid represents a color:\n",
    "0 = black, 1 = blue, 2 = red, 3 = green, 4 = yellow,\n",
    "5 = grey, 6 = pink, 7 = orange, 8 = light blue, 9 = brown.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following prompt tasks the LLM with program generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"\"\"\n",
    "Write a Python function that correctly transforms each input grid into its corresponding output grid based on the given examples.\n",
    "\n",
    "- ONLY return code. No explanations or anything other than code.\n",
    "- The function must be named: `solve(grid: List[List[int]]) -> List[List[int]]`\n",
    "- Use only pure Python — do not import or use libraries like NumPy\n",
    "- Do not include comments, explanations, or print statements\n",
    "- Do not hard-code values or specific grid sizes — the function must generalize based on the patterns in the examples\n",
    "- The function must return a plain 2D list of integers with consistent row lengths (List[List[int]])\n",
    "- Do not return arrays, nested arrays, floats, or 3D structures\n",
    "- Ensure your solution works for all provided input-output pairs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Write a Python function that correctly transforms each input grid into its corresponding output grid based on the given examples.\n",
      "\n",
      "- ONLY return code. No explanations or anything other than code.\n",
      "- The function must be named: `solve(grid: List[List[int]]) -> List[List[int]]`\n",
      "- Use only pure Python — do not import or use libraries like NumPy\n",
      "- Do not include comments, explanations, or print statements\n",
      "- Do not hard-code values or specific grid sizes — the function must generalize based on the patterns in the examples\n",
      "- The function must return a plain 2D list of integers with consistent row lengths (List[List[int]])\n",
      "- Do not return arrays, nested arrays, floats, or 3D structures\n",
      "- Ensure your solution works for all provided input-output pairs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(base_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the tasks from the specified folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tasks(folder):\n",
    "    tasks = []\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        if filename.endswith(\".json\"):\n",
    "            with open(os.path.join(folder, filename), \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                tasks.append({\"filename\": filename, \"data\": data})\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load API-Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the API-Key from the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_api_key(file_path=\"key.env\"):\n",
    "    load_dotenv(file_path)\n",
    "    import openai\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not openai.api_key:\n",
    "        print(\"No API key found. Please set OPENAI_API_KEY in key.env.\")\n",
    "    global client\n",
    "    client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used to send requests to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import openai\n",
    "\n",
    "def call_gpt(prompt, model=\"o4-mini\", retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                # Only for GPT-4o\n",
    "                # temperature=0.0\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        \n",
    "        except openai.RateLimitError as e:\n",
    "            wait_time = 5 + attempt * 5  # exponential backoff\n",
    "            print(f\"Rate limit hit. Waiting {wait_time} seconds before retrying...\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "    raise Exception(\"Rate limit retries exhausted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Combining Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Tasks to the Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adds the task data to the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tasks(prompt, task_data):\n",
    "    full_prompt = prompt.strip() + \"\\n\\nHere are the demonstration pairs (JSON data):\\n\"\n",
    "    for i, pair in enumerate(task_data['train']):\n",
    "        full_prompt += f\"\\nTrain Input {i+1}: {pair['input']}\\n\"\n",
    "        full_prompt += f\"Train Output {i+1}: {pair['output']}\\n\"\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function saves the generated programs in the specified folder using the tasks name. Additionally the LLMs response is cleaned ensuring only runnable Python code is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_program(program_text, actual_task_id, suffix=\"\"):\n",
    "    # Define the base and task-specific folder paths\n",
    "    base_folder = \"Candidate_programs_basic_prompts_4\"\n",
    "    task_folder = os.path.join(base_folder, actual_task_id)\n",
    "    \n",
    "    # Create the task-specific folder if it doesn't exist\n",
    "    os.makedirs(task_folder, exist_ok=True)\n",
    "\n",
    "    # Remove ```python or ``` if present\n",
    "    cleaned_text = re.sub(r\"^```(?:python)?\\s*|```$\", \"\", program_text.strip(), flags=re.MULTILINE)\n",
    "\n",
    "    # Find the next available version number, excluding suffix for base counting\n",
    "    existing_files = os.listdir(task_folder)\n",
    "    version_numbers = [\n",
    "        int(re.search(r\"solution_v(\\d+)\", fname).group(1))\n",
    "        for fname in existing_files\n",
    "        if re.match(r\"solution_v\\d+\", fname)\n",
    "    ]\n",
    "    next_version = max(version_numbers, default=0) + 1\n",
    "    \n",
    "    # Define the full path to the new Python file with the suffix (if provided)\n",
    "    file_path = os.path.join(task_folder, f\"solution_v{next_version}{suffix}.py\")\n",
    "    \n",
    "    # Save the program text to the file\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cleaned_text.strip())\n",
    "\n",
    "    print(f\"Saved program for task {actual_task_id} as version {next_version}{suffix}: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function passes the prompt for program creation to the LLM for n amount of times and saves the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_programs(tailored_prompt, actual_task_id, amount):\n",
    "    # Create two programs (change range for n programs)\n",
    "    for i in range(amount):  # You can adjust the range to create more programs\n",
    "        response = call_gpt(tailored_prompt)\n",
    "        \n",
    "        # Save the generated program\n",
    "        save_program(response, actual_task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads and executes a Python program from a specified file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_program(file_path):\n",
    "    \"\"\"Load and execute a Python program from a file.\"\"\"\n",
    "    spec = importlib.util.spec_from_file_location(\"program\", file_path)\n",
    "    program = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(program)\n",
    "    return program.solve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks if the program is valid Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_python_code(filepath):\n",
    "    \"\"\"Check if the Python file contains valid syntax.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            source = f.read()\n",
    "        ast.parse(source)\n",
    "        return True\n",
    "    except SyntaxError:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function runs the evaluation of the generated programs. This includes:\n",
    "\n",
    "- Checking if the program is valid Python code and deleting it if it isn't. \n",
    "- Checking if there are any valid programs among the generated ones (if there aren't at least two valid programs, more will be created until there are at least two [correctness of programs doesn't matter only execution])\n",
    "- Comparing the generated outputs with the correct outputs.\n",
    "- Calculating a score for each program.\n",
    "\n",
    "The score is defined by the amount of correct transformations / the total amount of demonstration pairs of a task. This score is used to determine whether a program should be revised later on (score < 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_programs(task_data, task_folder):\n",
    "    \"\"\"Evaluate programs and collect detailed candidate outputs for each demonstration pair.\"\"\"\n",
    "    programs = []\n",
    "    program_files = [f for f in os.listdir(task_folder) if f.endswith(\".py\")]\n",
    "\n",
    "    # Track whether any valid programs exist\n",
    "    any_valid = False\n",
    "\n",
    "    for program_file in program_files:\n",
    "        program_path = os.path.join(task_folder, program_file)\n",
    "\n",
    "        # Check if the program is valid Python code\n",
    "        if not is_valid_python_code(program_path):\n",
    "            print(f\"Deleting invalid file: {program_file}\")\n",
    "            os.remove(program_path)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            solve_function = load_program(program_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading program {program_file}: {e}\")\n",
    "            os.remove(program_path)\n",
    "            continue\n",
    "\n",
    "        # Set to true if at least one valid program is found\n",
    "        any_valid = True\n",
    "\n",
    "        details = []\n",
    "        correct_count = 0\n",
    "        total_pairs = len(task_data['train'])\n",
    "\n",
    "        for pair in task_data['train']:\n",
    "            input_grid = pair['input']\n",
    "            expected_output = pair['output']\n",
    "            try:\n",
    "                candidate_output = solve_function(input_grid)\n",
    "                if np.array_equal(np.array(candidate_output), np.array(expected_output)):\n",
    "                    correct_count += 1\n",
    "            except Exception as e:\n",
    "                candidate_output = f\"Error: {e}\"\n",
    "            details.append({\n",
    "                \"input\": input_grid,\n",
    "                \"candidate_output\": candidate_output,\n",
    "                \"expected_output\": expected_output\n",
    "            })\n",
    "\n",
    "        score = correct_count / total_pairs if total_pairs > 0 else 0\n",
    "        programs.append({\n",
    "            \"program_name\": program_file,\n",
    "            \"score\": score,\n",
    "            \"correct_pairs\": correct_count,\n",
    "            \"total_pairs\": total_pairs,\n",
    "            \"details\": details\n",
    "        })\n",
    "\n",
    "    return programs if any_valid else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of Predictions on Test Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identification of Best Programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selects the two best performing programs based on their score (= performance on the demonstration pairs). If scores are equal over multiple programs the first few programs will be picked (e.g. if all are 0 then solution_v1 and solution_v2 will be picked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_programs(evaluation_results, actual_task_id, n=2):\n",
    "    \"\"\"\n",
    "    Evaluates programs for a given task and returns the file paths for the top n programs based on demonstration pairs.\n",
    "    Always returns exactly n programs by sorting by score in descending order.\n",
    "    \"\"\"\n",
    "    # Sort programs by score descending; if scores are equal, the original order is preserved.\n",
    "    sorted_programs = sorted(evaluation_results, key=lambda x: x['score'], reverse=True)\n",
    "    task_folder = os.path.join(\"Candidate_programs_basic_prompts_4\", actual_task_id)\n",
    "    best_program_files = [os.path.join(task_folder, prog['program_name']) for prog in sorted_programs[:n]]\n",
    "    for i in best_program_files:\n",
    "        print(f\"Best program: {i}\")\n",
    "    return best_program_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of Predictions on Test Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function loads the two best performing programs to create predictions in the test inputs of the task. The resulting outputs are saved in a submission dictionary to be appended to the submission.json file later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_predictions(task_data, actual_task_id, best_program_files):\n",
    "    \"\"\"\n",
    "    Loads the two best candidate programs from the specified file paths,\n",
    "    runs each one on every test input, and saves the generated outputs\n",
    "    in the submission file.\n",
    "    \n",
    "    Expects task_data to have a 'test' key with a list of test pairs,\n",
    "    where each pair is a dict containing an \"input\" key.\n",
    "    \"\"\"\n",
    "    # Load the candidate solvers using the file paths.\n",
    "    best_solvers = [load_program(prog_file) for prog_file in best_program_files]\n",
    "    \n",
    "    predictions = []\n",
    "    # Iterate over each test pair.\n",
    "    for i, pair in enumerate(task_data[\"test\"]):\n",
    "        input_grid = pair[\"input\"]\n",
    "        attempt_predictions = {}\n",
    "        # Run each candidate solver on the test input.\n",
    "        for idx, solver in enumerate(best_solvers, start=1):\n",
    "            try:\n",
    "                output = solver(input_grid)\n",
    "            except Exception as e:\n",
    "                output = f\"Error: {e}\"\n",
    "            attempt_predictions[f\"attempt_{idx}\"] = output\n",
    "        predictions.append(attempt_predictions)\n",
    "    \n",
    "    submission = {str(actual_task_id): predictions}\n",
    "    \n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the flow of the Pipeline. All the above functions and prompts are used and work together to create predictions for test inputs. The predictions are saved in a submission.json file for calculating the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved program for task 0c786b71 as version 3: Candidate_programs_basic_prompts_4\\0c786b71\\solution_v3.py\n",
      "Saved program for task 0c786b71 as version 4: Candidate_programs_basic_prompts_4\\0c786b71\\solution_v4.py\n",
      "Task 0c786b71 evaluation results:\n",
      "Program solution_v1.py solved 3 out of 3 pairs. Score: 1.00\n",
      "==================================================\n",
      "Program solution_v2.py solved 3 out of 3 pairs. Score: 1.00\n",
      "==================================================\n",
      "Program solution_v3.py solved 3 out of 3 pairs. Score: 1.00\n",
      "==================================================\n",
      "Program solution_v4.py solved 3 out of 3 pairs. Score: 1.00\n",
      "==================================================\n",
      "Best program: Candidate_programs_basic_prompts_4\\0c786b71\\solution_v1.py\n",
      "Best program: Candidate_programs_basic_prompts_4\\0c786b71\\solution_v2.py\n",
      "Saved program for task 137f0df0 as version 1: Candidate_programs_basic_prompts_4\\137f0df0\\solution_v1.py\n",
      "Saved program for task 137f0df0 as version 2: Candidate_programs_basic_prompts_4\\137f0df0\\solution_v2.py\n",
      "Task 137f0df0 evaluation results:\n",
      "Program solution_v1.py solved 3 out of 3 pairs. Score: 1.00\n",
      "==================================================\n",
      "Program solution_v2.py solved 3 out of 3 pairs. Score: 1.00\n",
      "==================================================\n",
      "Best program: Candidate_programs_basic_prompts_4\\137f0df0\\solution_v1.py\n",
      "Best program: Candidate_programs_basic_prompts_4\\137f0df0\\solution_v2.py\n"
     ]
    }
   ],
   "source": [
    "# Load tasks and API key\n",
    "tasks = load_tasks(\"evaluation_set\")\n",
    "load_api_key()\n",
    "\n",
    "\n",
    "# Load existing submission file if it exists\n",
    "submission_file = \"submission_basic_prompts_4.json\"\n",
    "if os.path.exists(submission_file):\n",
    "    with open(submission_file, \"r\") as f:\n",
    "        final_submission = json.load(f)\n",
    "else:\n",
    "    final_submission = {}\n",
    "\n",
    "\n",
    "# Loop through each task (adjustable range)\n",
    "for i, task in enumerate(tasks[4:6]):\n",
    "    actual_task_id = task[\"filename\"].split(\".\")[0]\n",
    "    \n",
    "    ### PROMPT CREATION ###\n",
    "    full_prompt = add_tasks(base_prompt, task['data'])\n",
    "    create_programs(full_prompt, actual_task_id, amount=2)  # Create first 2 programs\n",
    "    \n",
    "    \n",
    "    ### INITIAL EVALUATION ###\n",
    "    task_folder = os.path.join(\"Candidate_programs_basic_prompts_4\", actual_task_id)\n",
    "    evaluation_results = evaluate_programs(task['data'], task_folder)\n",
    "    \n",
    "    \n",
    "    # If all programs are invalid, retry creating programs\n",
    "    while evaluation_results == 0:\n",
    "        create_programs(full_prompt, actual_task_id, amount=2)\n",
    "        evaluation_results = evaluate_programs(task['data'], task_folder)\n",
    "    \n",
    "    \n",
    "    ### CONTINUED PROGRAM CREATION UNTIL 2 VALID OR 6 TOTAL PROGRAMS ###\n",
    "    correct_programs = sum(1 for result in evaluation_results if result['score'] == 1.0)\n",
    "    number_of_programs = len(evaluation_results)\n",
    "\n",
    "    while (correct_programs < 2) and (number_of_programs < 4):\n",
    "        create_programs(full_prompt, actual_task_id, amount=2)\n",
    "        evaluation_results = evaluate_programs(task['data'], task_folder)\n",
    "        \n",
    "        if evaluation_results == 0:\n",
    "            continue\n",
    "\n",
    "        correct_programs = sum(1 for result in evaluation_results if result['score'] == 1.0)\n",
    "        number_of_programs = len(evaluation_results)\n",
    "        \n",
    "        \n",
    "    ### FINAL EVALUATION ###\n",
    "    evaluation_results_3 = evaluate_programs(task['data'], task_folder)\n",
    "    print(f\"Task {actual_task_id} evaluation results:\")\n",
    "    for result in evaluation_results_3:\n",
    "        print(f\"Program {result['program_name']} solved {result['correct_pairs']} out of {result['total_pairs']} pairs. Score: {result['score']:.2f}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "\n",
    "    ### PROGRAM SELECTION + PREDICTIONS ###\n",
    "    best_program_files = get_best_programs(evaluation_results_3, actual_task_id, n=2)\n",
    "    submission = generate_test_predictions(task['data'], actual_task_id, best_program_files)\n",
    "    final_submission.update(submission)\n",
    "\n",
    "\n",
    "# Final output file\n",
    "with open(submission_file, \"w\") as f:\n",
    "    json.dump(final_submission, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Accuracy (for personal use if test outputs are present)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compares the submission.json file with the actual correct test outputs of a task and returns the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_equal(a, b):\n",
    "    return json.dumps(a) == json.dumps(b)\n",
    "\n",
    "def compute_accuracy(submission_path, tasks_dict):\n",
    "    with open(submission_path, 'r') as f:\n",
    "        submission = json.load(f)\n",
    "\n",
    "    total_tasks = len(submission)\n",
    "    correct_tasks = 0\n",
    "\n",
    "    for task_id, prediction_list in submission.items():\n",
    "        task_data = tasks_dict[task_id]\n",
    "        test_outputs = [test['output'] for test in task_data['test']]\n",
    "\n",
    "        all_test_cases_correct = True\n",
    "\n",
    "        for idx, expected_output in enumerate(test_outputs):\n",
    "            attempts = prediction_list[idx]  # get dict with attempt_1 and attempt_2 for this test input\n",
    "            pred1 = attempts[\"attempt_1\"]\n",
    "            pred2 = attempts[\"attempt_2\"]\n",
    "\n",
    "            if not (are_equal(pred1, expected_output) or are_equal(pred2, expected_output)):\n",
    "                all_test_cases_correct = False\n",
    "                break\n",
    "\n",
    "        if all_test_cases_correct:\n",
    "            correct_tasks += 1\n",
    "\n",
    "    accuracy = correct_tasks / total_tasks\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "with open(\"submission_basic_prompts_4.json\") as f:\n",
    "    submission = json.load(f)\n",
    "\n",
    "tasks_dict = {task[\"filename\"].split(\".\")[0]: task[\"data\"] for task in tasks}\n",
    "\n",
    "accuracy = compute_accuracy(\"submission_basic_prompts_4.json\", tasks_dict)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a Python function that correctly transforms each input grid into its corresponding output grid based on the given examples.\n",
      "\n",
      "- ONLY return code. No explanations or anything other than code.\n",
      "- The function must be named: `solve(grid: List[List[int]]) -> List[List[int]]`\n",
      "- Use only pure Python — do not import or use libraries like NumPy\n",
      "- Do not include comments, explanations, or print statements\n",
      "- Do not hard-code values or specific grid sizes — the function must generalize based on the patterns in the examples\n",
      "- The function must return a plain 2D list of integers with consistent row lengths (List[List[int]])\n",
      "- Do not return arrays, nested arrays, floats, or 3D structures\n",
      "- Ensure your solution works for all provided input-output pairs\n",
      "\n",
      "Here are the demonstration pairs (JSON data):\n",
      "\n",
      "Train Input 1: [[5, 5, 0, 5, 5, 0, 5, 5, 0, 0], [5, 5, 0, 5, 5, 0, 5, 5, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [5, 5, 0, 5, 5, 0, 5, 5, 0, 0], [5, 5, 0, 5, 5, 0, 5, 5, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [5, 5, 0, 5, 5, 0, 5, 5, 0, 0], [5, 5, 0, 5, 5, 0, 5, 5, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "Train Output 1: [[5, 5, 2, 5, 5, 2, 5, 5, 0, 0], [5, 5, 2, 5, 5, 2, 5, 5, 0, 0], [2, 2, 2, 2, 2, 2, 2, 2, 1, 1], [5, 5, 2, 5, 5, 2, 5, 5, 0, 0], [5, 5, 2, 5, 5, 2, 5, 5, 0, 0], [2, 2, 2, 2, 2, 2, 2, 2, 1, 1], [5, 5, 2, 5, 5, 2, 5, 5, 0, 0], [5, 5, 2, 5, 5, 2, 5, 5, 0, 0], [0, 0, 1, 0, 0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 1, 0, 0, 0, 0]]\n",
      "\n",
      "Train Input 2: [[0, 0, 5, 5, 0, 0, 5, 5, 0, 0], [0, 0, 5, 5, 0, 0, 5, 5, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 5, 5, 0, 0, 5, 5, 0, 0], [0, 0, 5, 5, 0, 0, 5, 5, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 5, 5, 0, 0, 5, 5, 0, 0], [0, 0, 5, 5, 0, 0, 5, 5, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "Train Output 2: [[0, 0, 5, 5, 2, 2, 5, 5, 0, 0], [0, 0, 5, 5, 2, 2, 5, 5, 0, 0], [1, 1, 2, 2, 2, 2, 2, 2, 1, 1], [0, 0, 5, 5, 2, 2, 5, 5, 0, 0], [0, 0, 5, 5, 2, 2, 5, 5, 0, 0], [1, 1, 2, 2, 2, 2, 2, 2, 1, 1], [0, 0, 5, 5, 2, 2, 5, 5, 0, 0], [0, 0, 5, 5, 2, 2, 5, 5, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0, 0, 0]]\n",
      "\n",
      "Train Input 3: [[0, 5, 5, 0, 5, 5, 0, 5, 5, 0], [0, 5, 5, 0, 5, 5, 0, 5, 5, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 5, 5, 0, 5, 5, 0, 5, 5, 0], [0, 5, 5, 0, 5, 5, 0, 5, 5, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 5, 5, 0, 5, 5, 0, 5, 5, 0], [0, 5, 5, 0, 5, 5, 0, 5, 5, 0]]\n",
      "Train Output 3: [[0, 5, 5, 2, 5, 5, 2, 5, 5, 0], [0, 5, 5, 2, 5, 5, 2, 5, 5, 0], [1, 2, 2, 2, 2, 2, 2, 2, 2, 1], [1, 2, 2, 2, 2, 2, 2, 2, 2, 1], [0, 5, 5, 2, 5, 5, 2, 5, 5, 0], [0, 5, 5, 2, 5, 5, 2, 5, 5, 0], [1, 2, 2, 2, 2, 2, 2, 2, 2, 1], [1, 2, 2, 2, 2, 2, 2, 2, 2, 1], [0, 5, 5, 2, 5, 5, 2, 5, 5, 0], [0, 5, 5, 2, 5, 5, 2, 5, 5, 0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(full_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
